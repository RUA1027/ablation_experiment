import torch
import torch.nn as nn
import torch.optim as optim
import os
import time
from typing import Any, Mapping, Optional, Dict, Union

# TensorBoard æ”¯æŒ
try:
    # å°è¯•ä» writer å­æ¨¡å—å¯¼å…¥ (è§£å†³ IDE é™æ€æ£€æŸ¥æŠ¥é”™)
    from torch.utils.tensorboard.writer import SummaryWriter as _SummaryWriter
    SummaryWriter = _SummaryWriter
    TENSORBOARD_AVAILABLE = True
except ImportError:
    try:
        # å›é€€åˆ°æ ‡å‡†å¯¼å…¥æ–¹å¼
        from torch.utils.tensorboard import SummaryWriter as _SummaryWriter  # type: ignore[attr-defined]
        SummaryWriter = _SummaryWriter
        TENSORBOARD_AVAILABLE = True
    except ImportError:
        TENSORBOARD_AVAILABLE = False
        SummaryWriter = None

'''
================================================================================
                    ä¸‰é˜¶æ®µè§£è€¦è®­ç»ƒç­–ç•¥ (Three-Stage Decoupled Training)
================================================================================

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 1: Physics Only (ç‰©ç†å±‚å•ç‹¬è®­ç»ƒ) - 50 Epochs                          â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç›®çš„: åˆ©ç”¨æˆå¯¹æ•°æ®ï¼Œå•ç‹¬è®­ç»ƒ AberrationNet å‡†ç¡®æ‹Ÿåˆæ•°æ®é›†çš„å…‰å­¦åƒå·®ç‰¹æ€§     â”‚
â”‚                                                                              â”‚
â”‚  æ•°æ®æµ:                                                                     â”‚
â”‚    X_gt (æ¸…æ™°å›¾åƒ) â”€â”€â–¶ PhysicalLayer â”€â”€â–¶ Y_hat (é‡æ¨¡ç³Š)                     â”‚
â”‚                                                                              â”‚
â”‚  Loss = MSE(Y_hat, Y) + Î»_coeff Ã— ||coeffs||Â² + Î»_smooth Ã— TV(coeffs)       â”‚
â”‚                                                                              â”‚
â”‚  å†»ç»“: RestorationNet (â„ï¸)     æ›´æ–°: AberrationNet (ğŸ”¥)                      â”‚
â”‚  éªŒè¯åˆ¤æ®: Re-blur MSE (é‡æ¨¡ç³Šä¸€è‡´æ€§è¯¯å·®)                                    â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 2: Restoration with Fixed Physics (å›ºå®šç‰©ç†å±‚è®­ç»ƒå¤åŸç½‘ç»œ) - 200 Epochsâ”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç›®çš„: åœ¨å·²çŸ¥ä¸”å‡†ç¡®çš„ç‰©ç†æ¨¡å‹æŒ‡å¯¼ä¸‹ï¼Œè®­ç»ƒå¤åŸç½‘ç»œ                            â”‚
â”‚                                                                              â”‚
â”‚  æ•°æ®æµ:                                                                     â”‚
â”‚    Y (æ¨¡ç³Šå›¾åƒ) â”€â”€â–¶ RestorationNet â”€â”€â–¶ X_hat â”€â”€â–¶ PhysicalLayer â”€â”€â–¶ Y_hat   â”‚
â”‚                                                                              â”‚
â”‚  Loss = Î»_sup Ã— L1(X_hat, X_gt) + MSE(Y_hat, Y) + Î»_image_reg Ã— TV(X_hat)  â”‚
â”‚                                                                              â”‚
â”‚  å†»ç»“: AberrationNet (â„ï¸)      æ›´æ–°: RestorationNet (ğŸ”¥)                     â”‚
â”‚  éªŒè¯åˆ¤æ®: Validation PSNR & SSIM                                            â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜

â”Œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”
â”‚  Stage 3: Joint Fine-tuning (è”åˆå¾®è°ƒ) - 50 Epochs                           â”‚
â”œâ”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”¤
â”‚  ç›®çš„: è”åˆå¾®è°ƒï¼Œæ¶ˆé™¤æ¨¡å—é—´çš„è€¦åˆè¯¯å·®                                        â”‚
â”‚                                                                              â”‚
â”‚  æ•°æ®æµ:                                                                     â”‚
â”‚    Y â”€â”€â–¶ RestorationNet â”€â”€â–¶ X_hat â”€â”€â–¶ PhysicalLayer â”€â”€â–¶ Y_hat              â”‚
â”‚                                                                              â”‚
â”‚  Loss = ç»¼åˆæŸå¤±ï¼ˆæ‰€æœ‰é¡¹ï¼‰                                                   â”‚
â”‚  å­¦ä¹ ç‡: å‡åŠ (lr_restoration / 2, lr_optics / 2)                           â”‚
â”‚                                                                              â”‚
â”‚  æ›´æ–°: RestorationNet (ğŸ”¥) + AberrationNet (ğŸ”¥)                              â”‚
â”‚  éªŒè¯åˆ¤æ®: ç»¼åˆæŒ‡æ ‡ (PSNR + ç‰©ç†çº¦æŸ)                                        â”‚
â””â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”€â”˜
'''
class DualBranchTrainer:
    """
    ä¸‰é˜¶æ®µè§£è€¦è®­ç»ƒå™¨ (Three-Stage Decoupled Trainer)

    æ”¯æŒä¸‰ç§è®­ç»ƒæ¨¡å¼:
    - 'physics_only': ä»…è®­ç»ƒç‰©ç†å±‚ (Stage 1)
    - 'restoration_fixed_physics': å›ºå®šç‰©ç†å±‚è®­ç»ƒå¤åŸç½‘ç»œ (Stage 2)
    - 'joint': è”åˆè®­ç»ƒæ‰€æœ‰æ¨¡å— (Stage 3)
    
    ç‰¹æ€§:
    - åŠ¨æ€æŸå¤±æƒé‡è°ƒæ•´
    - ç†”æ–­æœºåˆ¶ (Circuit Breaker)
    - TensorBoard æ—¥å¿—
    - Stage 3 å­¦ä¹ ç‡è‡ªåŠ¨å‡åŠ
    """

    VALID_STAGES = ('physics_only', 'restoration_fixed_physics', 'joint', 'restoration_only')

    def __init__(self,
                 restoration_net,
                 physical_layer,
                 lr_restoration,
                 lr_optics,
                 optimizer_type="adamw",
                 weight_decay=0.0,
                 lambda_sup=1.0,
                 lambda_coeff=0.05,
                 lambda_smooth=0.1,
                 lambda_image_reg=0.0,
                 grad_clip_restoration=5.0,
                 grad_clip_optics=1.0,
                 stage_schedule=None,
                 stage_weights=None,
                 smoothness_grid_size=16,
                 device='cuda',
                 accumulation_steps=4,
                 tensorboard_dir=None,
                 circuit_breaker_config=None):

        self.device = device
        self.restoration_net = restoration_net.to(device)
        self.physical_layer = physical_layer.to(device) if physical_layer is not None else None
        self.use_physical_layer = self.physical_layer is not None

        # Access internals for regularization
        self.aberration_net = self.physical_layer.aberration_net if self.physical_layer is not None else None

        # ä¿å­˜åŸå§‹å­¦ä¹ ç‡ç”¨äº Stage 3 å‡åŠ
        self.base_lr_restoration = lr_restoration
        self.base_lr_optics = lr_optics
        
        # ç‹¬ç«‹ä¼˜åŒ–å™¨
        opt_type = str(optimizer_type).lower()
        if opt_type == "adamw":
            optimizer_cls = optim.AdamW
        elif opt_type == "adam":
            optimizer_cls = optim.Adam
        else:
            raise ValueError(f"Unsupported optimizer type: {optimizer_type}")

        self.optimizer_W = optimizer_cls(self.restoration_net.parameters(), lr=lr_restoration, weight_decay=weight_decay)
        self.optimizer_Theta = None
        if self.aberration_net is not None:
            self.optimizer_Theta = optimizer_cls(self.aberration_net.parameters(), lr=lr_optics, weight_decay=weight_decay)

        # å…¼å®¹æ—§é…ç½®ï¼ˆå·²å¼ƒç”¨çš„å›ºå®šæƒé‡ï¼Œä»…ä¿ç•™å­—æ®µï¼‰
        self.lambda_sup = lambda_sup
        self.lambda_coeff = lambda_coeff
        self.lambda_smooth = lambda_smooth
        self.lambda_image_reg = lambda_image_reg

        self.stage_weights = stage_weights if stage_weights is not None else {}
        
        # ä¸‰é˜¶æ®µè°ƒåº¦ (å¯ä¸º dict æˆ– dataclass)
        default_schedule = {
            'stage1_epochs': 50,
            'stage2_epochs': 200,
            'stage3_epochs': 50
        }
        self.stage_schedule: Any = stage_schedule if stage_schedule is not None else default_schedule

        # å¹³æ»‘æ­£åˆ™é‡‡æ ·ç½‘æ ¼å¤§å°
        self.smoothness_grid_size = smoothness_grid_size

        # æ¢¯åº¦ç´¯ç§¯
        self.accumulation_steps = max(1, accumulation_steps)
        self.accumulation_counter = 0

        # æ¢¯åº¦è£å‰ªé˜ˆå€¼
        self.grad_clip_restoration = grad_clip_restoration
        self.grad_clip_optics = grad_clip_optics

        # æŸå¤±å‡½æ•°
        self.criterion_mse = nn.MSELoss()
        self.criterion_l1 = nn.L1Loss()

        # å½“å‰è®­ç»ƒé˜¶æ®µ
        self._current_stage = 'joint' if self.use_physical_layer else 'restoration_only'
        self._previous_stage = None  # ç”¨äºæ£€æµ‹é˜¶æ®µåˆ‡æ¢
        self._stage3_lr_halved = False  # æ ‡è®° Stage 3 å­¦ä¹ ç‡æ˜¯å¦å·²å‡åŠ
        self._forced_stage = None  # å¼ºåˆ¶é˜¶æ®µ (ç”¨äºç†”æ–­é˜»æ–­åˆ‡æ¢)

        # ç†”æ–­æœºåˆ¶é…ç½®
        self.circuit_breaker_config = circuit_breaker_config or {
            'enabled': True,
            'stage1_min_loss': 0.005,
            'stage2_min_psnr': 30.0
        }
        self.circuit_breaker_triggered = False
        self.circuit_breaker_message = ""

        # TensorBoard
        self.writer = None  # type: Optional[Any]
        if tensorboard_dir and TENSORBOARD_AVAILABLE and SummaryWriter is not None:
            self.writer = SummaryWriter(log_dir=tensorboard_dir)
            print(f"[TensorBoard] Logging to: {tensorboard_dir}")
        elif tensorboard_dir and not TENSORBOARD_AVAILABLE:
            print("[Warning] TensorBoard not available. Install with: pip install tensorboard")

        # History
        self.history = {
            'loss_total': [], 'loss_data': [], 'loss_sup': [],
            'grad_norm_W': [], 'grad_norm_Theta': []
        }
        
        # å„é˜¶æ®µæœ€ä½³éªŒè¯æŒ‡æ ‡
        self.best_metrics = {
            'physics_only': {'reblur_mse': float('inf')},
            'restoration_fixed_physics': {'psnr': 0.0, 'ssim': 0.0},
            'joint': {'psnr': 0.0, 'combined': 0.0},
            'restoration_only': {'psnr': 0.0}
        }

    # =========================================================================
    #                          é˜¶æ®µè°ƒåº¦ä¸å†»ç»“ç­–ç•¥
    # =========================================================================
    def _get_stage(self, epoch: int) -> str:
        """æ ¹æ® epoch(0-indexed) è·å–å½“å‰é˜¶æ®µ"""
        if not self.use_physical_layer:
            return 'restoration_only'
        if isinstance(self.stage_schedule, Mapping):
            s1 = self.stage_schedule.get('stage1_epochs', 50)
            s2 = self.stage_schedule.get('stage2_epochs', 200)
        else:
            s1 = getattr(self.stage_schedule, 'stage1_epochs', 50)
            s2 = getattr(self.stage_schedule, 'stage2_epochs', 200)

        if epoch < s1:
            return 'physics_only'
        elif epoch < s1 + s2:
            return 'restoration_fixed_physics'
        return 'joint'

    def _adjust_learning_rate_for_stage3(self):
        """Stage 3 å­¦ä¹ ç‡å‡åŠ"""
        if not self.use_physical_layer:
            return
        if self._stage3_lr_halved:
            return  # å·²ç»è°ƒæ•´è¿‡
        
        new_lr_W = self.base_lr_restoration / 2.0
        new_lr_Theta = self.base_lr_optics / 2.0
        
        for param_group in self.optimizer_W.param_groups:
            param_group['lr'] = new_lr_W
        if self.optimizer_Theta is not None:
            for param_group in self.optimizer_Theta.param_groups:
                param_group['lr'] = new_lr_Theta
        
        self._stage3_lr_halved = True
        print(f"[Stage 3] Learning rate halved: lr_restoration={new_lr_W:.2e}, lr_optics={new_lr_Theta:.2e}")

    def check_circuit_breaker(self, val_metrics: Dict[str, float], current_stage: str, next_stage: str) -> bool:
        """
        ç†”æ–­æœºåˆ¶æ£€æŸ¥ï¼šéªŒè¯å½“å‰é˜¶æ®µæ˜¯å¦è¾¾åˆ°åˆ‡æ¢æ¡ä»¶
        
        Args:
            val_metrics: éªŒè¯é›†æŒ‡æ ‡å­—å…¸
            current_stage: å½“å‰è®­ç»ƒé˜¶æ®µ
            next_stage: å³å°†è¿›å…¥çš„é˜¶æ®µ
        
        Returns:
            bool: True è¡¨ç¤ºå¯ä»¥åˆ‡æ¢ï¼ŒFalse è¡¨ç¤ºç†”æ–­ï¼ˆä¸å…è®¸åˆ‡æ¢ï¼‰
        """
        if not self.use_physical_layer:
            return True
        if not self.circuit_breaker_config.get('enabled', False):
            return True  # ç†”æ–­æœºåˆ¶æœªå¯ç”¨ï¼Œå…è®¸åˆ‡æ¢
        
        # Stage 1 -> Stage 2: æ£€æŸ¥é‡æ¨¡ç³Šè¯¯å·®
        if current_stage == 'physics_only' and next_stage == 'restoration_fixed_physics':
            reblur_mse = val_metrics.get('Reblur_MSE', val_metrics.get('reblur_mse', float('inf')))
            threshold = self.circuit_breaker_config.get('stage1_min_loss', 0.5)
            
            if reblur_mse > threshold:
                self.circuit_breaker_triggered = True
                self.circuit_breaker_message = (
                    f"[Circuit Breaker] Stage 1 -> 2 BLOCKED: "
                    f"Reblur MSE ({reblur_mse:.4f}) > threshold ({threshold:.4f}). "
                    f"Physics layer not ready. Continuing Stage 1..."
                )
                return False
        
        # Stage 2 -> Stage 3: æ£€æŸ¥ PSNR å’Œ SSIM
        if current_stage == 'restoration_fixed_physics' and next_stage == 'joint':
            psnr = val_metrics.get('PSNR', val_metrics.get('psnr', 0.0))
            ssim = val_metrics.get('SSIM', val_metrics.get('ssim', 0.0))
            
            psnr_threshold = self.circuit_breaker_config.get('stage2_min_psnr', 20.0)
            ssim_threshold = self.circuit_breaker_config.get('stage2_min_ssim', 0.0)
            
            if psnr < psnr_threshold:
                self.circuit_breaker_triggered = True
                self.circuit_breaker_message = (
                    f"[Circuit Breaker] Stage 2 -> 3 BLOCKED: "
                    f"PSNR ({psnr:.2f}) < threshold ({psnr_threshold:.2f}). "
                    f"Restoration network not ready. Continuing Stage 2..."
                )
                return False

            if ssim < ssim_threshold:
                self.circuit_breaker_triggered = True
                self.circuit_breaker_message = (
                    f"[Circuit Breaker] Stage 2 -> 3 BLOCKED: "
                    f"SSIM ({ssim:.4f}) < threshold ({ssim_threshold:.4f}). "
                    f"Restoration network structural quality low. Continuing Stage 2..."
                )
                return False
        
        self.circuit_breaker_triggered = False
        self.circuit_breaker_message = ""
        return True

    def update_best_metrics(self, val_metrics: Dict[str, float], stage: str) -> Dict[str, bool]:
        """
        æ›´æ–°å„é˜¶æ®µæœ€ä½³æŒ‡æ ‡å¹¶åˆ¤æ–­æ˜¯å¦éœ€è¦ä¿å­˜æ¨¡å‹
        
        Returns:
            dict: å„æŒ‡æ ‡æ˜¯å¦ä¸ºæ–°æœ€ä½³å€¼
        """
        is_best = {}
        
        if stage == 'physics_only':
            reblur_mse = val_metrics.get('Reblur_MSE', val_metrics.get('reblur_mse', float('inf')))
            if reblur_mse < self.best_metrics['physics_only']['reblur_mse']:
                self.best_metrics['physics_only']['reblur_mse'] = reblur_mse
                is_best['reblur_mse'] = True
            else:
                is_best['reblur_mse'] = False
                
        elif stage == 'restoration_fixed_physics':
            psnr = val_metrics.get('PSNR', val_metrics.get('psnr', 0.0))
            ssim = val_metrics.get('SSIM', val_metrics.get('ssim', 0.0))
            
            if psnr > self.best_metrics['restoration_fixed_physics']['psnr']:
                self.best_metrics['restoration_fixed_physics']['psnr'] = psnr
                is_best['psnr'] = True
            else:
                is_best['psnr'] = False
                
            if ssim > self.best_metrics['restoration_fixed_physics']['ssim']:
                self.best_metrics['restoration_fixed_physics']['ssim'] = ssim
                is_best['ssim'] = True
            else:
                is_best['ssim'] = False
                
        elif stage == 'joint':
            psnr = val_metrics.get('PSNR', val_metrics.get('psnr', 0.0))
            reblur_mse = val_metrics.get('Reblur_MSE', val_metrics.get('reblur_mse', float('inf')))
            # ç»¼åˆæŒ‡æ ‡: PSNR è¶Šé«˜è¶Šå¥½ï¼ŒReblur_MSE è¶Šä½è¶Šå¥½
            # combined = PSNR - 10 * Reblur_MSE (ç»éªŒå…¬å¼)
            combined = psnr - 10.0 * reblur_mse
            
            if psnr > self.best_metrics['joint']['psnr']:
                self.best_metrics['joint']['psnr'] = psnr
                is_best['psnr'] = True
            else:
                is_best['psnr'] = False
                
            if combined > self.best_metrics['joint']['combined']:
                self.best_metrics['joint']['combined'] = combined
                is_best['combined'] = True
            else:
                is_best['combined'] = False
        elif stage == 'restoration_only':
            psnr = val_metrics.get('PSNR', val_metrics.get('psnr', 0.0))
            if psnr > self.best_metrics.get('restoration_only', {}).get('psnr', 0.0):
                self.best_metrics.setdefault('restoration_only', {})['psnr'] = psnr
                is_best['psnr'] = True
            else:
                is_best['psnr'] = False
        
        return is_best

    def log_to_tensorboard(self, metrics: Dict[str, float], epoch: int, prefix: str = 'train'):
        """è®°å½•æŒ‡æ ‡åˆ° TensorBoard"""
        if self.writer is None:
            return
        
        for key, value in metrics.items():
            if isinstance(value, (int, float)) and not (isinstance(value, float) and (value != value)):  # æ’é™¤ NaN
                self.writer.add_scalar(f'{prefix}/{key}', value, epoch)
    
    def log_gradients_to_tensorboard(self, epoch: int):
        """è®°å½•æ¢¯åº¦åˆ†å¸ƒåˆ° TensorBoard"""
        if self.writer is None:
            return
        
        # è®°å½•å¤åŸç½‘ç»œæ¢¯åº¦
        for name, param in self.restoration_net.named_parameters():
            if param.grad is not None:
                self.writer.add_histogram(f'gradients/restoration/{name}', param.grad, epoch)
        
        # è®°å½•åƒå·®ç½‘ç»œæ¢¯åº¦
        if self.aberration_net is not None:
            for name, param in self.aberration_net.named_parameters():
                if param.grad is not None:
                    self.writer.add_histogram(f'gradients/aberration/{name}', param.grad, epoch)
    
    def log_images_to_tensorboard(self, blur_img, sharp_img, restored_img, reblur_img, epoch: int):
        """è®°å½•å›¾åƒåˆ° TensorBoard"""
        if self.writer is None:
            return
        
        # åªå–ç¬¬ä¸€å¼ å›¾
        self.writer.add_image('images/blur', blur_img[0].clamp(0, 1), epoch)
        self.writer.add_image('images/sharp_gt', sharp_img[0].clamp(0, 1), epoch)
        self.writer.add_image('images/restored', restored_img[0].clamp(0, 1), epoch)
        if reblur_img is not None:
            self.writer.add_image('images/reblur', reblur_img[0].clamp(0, 1), epoch)
    
    def close_tensorboard(self):
        """å…³é—­ TensorBoard writer"""
        if self.writer is not None:
            self.writer.close()

    def _get_stage_weights(self, stage: str):
        """æ ¹æ®é˜¶æ®µè¿”å›åŠ¨æ€ Loss æƒé‡"""
        # é»˜è®¤æƒé‡
        weights = {
            'w_data': 1.0,
            'w_sup': 0.0,
            'w_smooth': 0.0,
            'w_coeff': 0.0,
            'w_img_reg': 0.0
        }

        # ä¼˜å…ˆä» stage_weights é…ç½®ä¸­è¯»å–
        # stage_weights åº”è¯¥æ˜¯ä¸€ä¸ªå­—å…¸ï¼ŒåŒ…å« 'physics_only', 'joint' ç­‰é”®
        if stage in self.stage_weights:
            custom_weights = self.stage_weights[stage]
            weights.update(custom_weights)
            return weights

        # Fallback åˆ°æ—§é€»è¾‘ (å¦‚æœé…ç½®ä¸­ä¸ºç©º)
        if stage == 'physics_only':
            weights.update({
                'w_data': 1.0, 
                'w_sup': 0.0, 
                'w_smooth': self.lambda_smooth, 
                'w_coeff': self.lambda_coeff, 
                'w_img_reg': 0.0
            })
        elif stage == 'restoration_fixed_physics':
            weights.update({
                'w_data': 0.1, 
                'w_sup': 1.0, 
                'w_smooth': 0.0, 
                'w_coeff': 0.0, 
                'w_img_reg': self.lambda_image_reg
            })
        elif stage == 'joint':
            weights.update({
                'w_data': 0.5, 
                'w_sup': 1.0, 
                'w_smooth': self.lambda_smooth * 0.5, 
                'w_coeff': self.lambda_coeff * 0.2, 
                'w_img_reg': self.lambda_image_reg * 0.1
            })
        elif stage == 'restoration_only':
            weights.update({
                'w_data': 0.0,
                'w_sup': 1.0,
                'w_smooth': 0.0,
                'w_coeff': 0.0,
                'w_img_reg': self.lambda_image_reg
            })

        return weights

    def _set_trainable(self, stage: str):
        """æ ¹æ®é˜¶æ®µå¿«é€Ÿå†»ç»“/è§£å†»ç½‘ç»œï¼Œå¹¶åˆ‡æ¢ train/eval æ¨¡å¼"""
        if stage == 'physics_only':
            for p in self.restoration_net.parameters():
                p.requires_grad = False
            if self.aberration_net is not None:
                for p in self.aberration_net.parameters():
                    p.requires_grad = True
            self.restoration_net.eval()
            if self.physical_layer is not None:
                self.physical_layer.train()
        elif stage == 'restoration_fixed_physics':
            for p in self.restoration_net.parameters():
                p.requires_grad = True
            if self.aberration_net is not None:
                for p in self.aberration_net.parameters():
                    p.requires_grad = False
            self.restoration_net.train()
            if self.physical_layer is not None:
                self.physical_layer.eval()
        elif stage == 'joint':
            for p in self.restoration_net.parameters():
                p.requires_grad = True
            if self.aberration_net is not None:
                for p in self.aberration_net.parameters():
                    p.requires_grad = True
            self.restoration_net.train()
            if self.physical_layer is not None:
                self.physical_layer.train()
        elif stage == 'restoration_only':
            for p in self.restoration_net.parameters():
                p.requires_grad = True
            self.restoration_net.train()
        else:
            raise ValueError(f"Invalid stage '{stage}'. Must be one of {self.VALID_STAGES}")

    def set_stage(self, stage: str):
        """å…¼å®¹æ—§æµç¨‹çš„æ‰‹åŠ¨è®¾ç½®ï¼ˆä»å¯ç”¨ï¼‰"""
        if stage not in self.VALID_STAGES:
            raise ValueError(f"Invalid stage '{stage}'. Must be one of {self.VALID_STAGES}")
        self._current_stage = stage
        self._set_trainable(stage)

    def get_stage(self, epoch: int) -> str:
        return self._resolve_stage(epoch)

    def set_forced_stage(self, stage: Optional[str]):
        if stage is None:
            self._forced_stage = None
            return
        if stage not in self.VALID_STAGES:
            raise ValueError(f"Invalid stage '{stage}'. Must be one of {self.VALID_STAGES}")
        self._forced_stage = stage

    def _resolve_stage(self, epoch: int) -> str:
        if self._forced_stage is not None:
            return self._forced_stage
        return self._get_stage(epoch)

    def get_stage_weights(self, epoch: int):
        return self._get_stage_weights(self._get_stage(epoch))

    # =========================================================================
    #                              æ ¸å¿ƒè®­ç»ƒæ­¥éª¤
    # =========================================================================
    def train_step(self, Y_blur, X_gt, epoch, crop_info=None):
        """
        æ‰§è¡Œä¸€ä¸ªè®­ç»ƒæ­¥éª¤ï¼Œå†…éƒ¨æ ¹æ® epoch è‡ªåŠ¨åˆ‡æ¢é˜¶æ®µå¹¶åˆ†é…åŠ¨æ€ Loss æƒé‡ã€‚
        """
        current_stage = self._resolve_stage(epoch)
        
        # æ£€æµ‹é˜¶æ®µåˆ‡æ¢
        if self._previous_stage is not None and self._previous_stage != current_stage:
            print(f"\n[Stage Transition] {self._previous_stage} -> {current_stage}")
            
            # Stage 3 å­¦ä¹ ç‡å‡åŠ
            if current_stage == 'joint' and not self._stage3_lr_halved:
                self._adjust_learning_rate_for_stage3()
        
        self._previous_stage = current_stage
        self._current_stage = current_stage
        self._set_trainable(current_stage)

        weights = self._get_stage_weights(current_stage)
        w_data = weights['w_data']
        w_sup = weights['w_sup']
        w_smooth = weights['w_smooth']
        w_coeff = weights['w_coeff']
        w_img_reg = weights['w_img_reg']

        Y_blur = Y_blur.to(self.device)
        X_gt = X_gt.to(self.device)
        if crop_info is not None:
            crop_info = crop_info.to(self.device)

        # æ¢¯åº¦ç´¯ç§¯ï¼šä»…åœ¨ç¬¬ä¸€ä¸ªç´¯ç§¯æ­¥éª¤æ¸…é™¤æ¢¯åº¦
        if self.accumulation_counter == 0:
            if not self.use_physical_layer:
                self.optimizer_W.zero_grad()
            elif current_stage == 'physics_only':
                if self.optimizer_Theta is not None:
                    self.optimizer_Theta.zero_grad()
            elif current_stage == 'restoration_fixed_physics':
                self.optimizer_W.zero_grad()
            else:
                self.optimizer_W.zero_grad()
                if self.optimizer_Theta is not None:
                    self.optimizer_Theta.zero_grad()

        # ========================== Forward & Loss ===========================
        loss_data = torch.tensor(0.0, device=self.device)
        loss_sup = torch.tensor(0.0, device=self.device)
        loss_coeff = torch.tensor(0.0, device=self.device)
        loss_smooth = torch.tensor(0.0, device=self.device)
        loss_image_reg = torch.tensor(0.0, device=self.device)

        if not self.use_physical_layer:
            X_hat = self.restoration_net(Y_blur)
            loss_sup = self.criterion_l1(X_hat, X_gt)
            if w_img_reg > 0:
                loss_image_reg = self.compute_image_tv_loss(X_hat)
        elif current_stage == 'physics_only':
            if self.physical_layer is None or self.aberration_net is None:
                raise RuntimeError("physical_layer and aberration_net are required for physics_only stage")
            Y_reblur = self.physical_layer(X_gt, crop_info=crop_info)
            loss_data = self.criterion_mse(Y_reblur, Y_blur)

            if w_coeff > 0 or w_smooth > 0:
                coords = self.physical_layer.get_patch_centers(
                    Y_blur.shape[2], Y_blur.shape[3], self.device
                )
                if coords.shape[0] > 64:
                    indices = torch.randperm(coords.shape[0])[:64]
                    coords = coords[indices]
                coeffs = self.aberration_net(coords)
                if w_coeff > 0:
                    loss_coeff = torch.mean(coeffs**2)
                if w_smooth > 0:
                    loss_smooth = self.physical_layer.compute_coefficient_smoothness(self.smoothness_grid_size)
        else:
            if self.physical_layer is None or self.aberration_net is None:
                raise RuntimeError("physical_layer and aberration_net are required for this stage")
            X_hat = self.restoration_net(Y_blur)
            Y_reblur = self.physical_layer(X_hat, crop_info=crop_info)
            loss_data = self.criterion_mse(Y_reblur, Y_blur)
            loss_sup = self.criterion_l1(X_hat, X_gt)

            if w_img_reg > 0:
                loss_image_reg = self.compute_image_tv_loss(X_hat)

            if current_stage == 'joint' and (w_coeff > 0 or w_smooth > 0):
                coords = self.physical_layer.get_patch_centers(
                    Y_blur.shape[2], Y_blur.shape[3], self.device
                )
                if coords.shape[0] > 64:
                    indices = torch.randperm(coords.shape[0])[:64]
                    coords = coords[indices]
                coeffs = self.aberration_net(coords)
                if w_coeff > 0:
                    loss_coeff = torch.mean(coeffs**2)
                if w_smooth > 0:
                    loss_smooth = self.physical_layer.compute_coefficient_smoothness(self.smoothness_grid_size)

        # ========================== Weighted Loss ============================
        loss_data_w = w_data * loss_data
        loss_sup_w = w_sup * loss_sup
        loss_coeff_w = w_coeff * loss_coeff
        loss_smooth_w = w_smooth * loss_smooth
        loss_image_reg_w = w_img_reg * loss_image_reg

        total_loss = loss_data_w + loss_sup_w + loss_coeff_w + loss_smooth_w + loss_image_reg_w

        scaled_loss = total_loss / self.accumulation_steps
        scaled_loss.backward()

        # ========================== Optimizer Step ============================
        self.accumulation_counter += 1
        should_step = (self.accumulation_counter >= self.accumulation_steps)

        gn_W = torch.tensor(0.0, device=self.device)
        gn_Theta = torch.tensor(0.0, device=self.device)

        if should_step:
            if not self.use_physical_layer:
                gn_W = nn.utils.clip_grad_norm_(self.restoration_net.parameters(), self.grad_clip_restoration)
                self.optimizer_W.step()
            elif current_stage == 'physics_only':
                if self.aberration_net is None:
                    raise RuntimeError("aberration_net is required for physics_only stage")
                gn_Theta = nn.utils.clip_grad_norm_(self.aberration_net.parameters(), self.grad_clip_optics)
                if self.optimizer_Theta is not None:
                    self.optimizer_Theta.step()
            elif current_stage == 'restoration_fixed_physics':
                gn_W = nn.utils.clip_grad_norm_(self.restoration_net.parameters(), self.grad_clip_restoration)
                self.optimizer_W.step()
            else:
                if self.aberration_net is None:
                    raise RuntimeError("aberration_net is required for joint stage")
                gn_W = nn.utils.clip_grad_norm_(self.restoration_net.parameters(), self.grad_clip_restoration)
                gn_Theta = nn.utils.clip_grad_norm_(self.aberration_net.parameters(), self.grad_clip_optics)
                self.optimizer_W.step()
                if self.optimizer_Theta is not None:
                    self.optimizer_Theta.step()

            self.accumulation_counter = 0

            self.history['loss_total'].append(total_loss.item())
            self.history['loss_data'].append(loss_data_w.item())
            self.history['grad_norm_W'].append(gn_W.item() if isinstance(gn_W, torch.Tensor) else gn_W)
            self.history['grad_norm_Theta'].append(gn_Theta.item() if isinstance(gn_Theta, torch.Tensor) else gn_Theta)

        return {
            'loss': total_loss.item(),
            'loss_data': loss_data_w.item(),
            'loss_sup': loss_sup_w.item(),
            'loss_coeff': loss_coeff_w.item(),
            'loss_smooth': loss_smooth_w.item(),
            'loss_image_reg': loss_image_reg_w.item(),
            'loss_data_raw': loss_data.item(),
            'loss_sup_raw': loss_sup.item(),
            'loss_coeff_raw': loss_coeff.item(),
            'loss_smooth_raw': loss_smooth.item(),
            'loss_image_reg_raw': loss_image_reg.item(),
            'grad_W': gn_W.item() if isinstance(gn_W, torch.Tensor) else gn_W,
            'grad_Theta': gn_Theta.item() if isinstance(gn_Theta, torch.Tensor) else gn_Theta,
            'stage': current_stage
        }

    def compute_image_tv_loss(self, img):
        """
        Compute Total Variation (TV) loss on the image.
        L_tv = mean(|dI/dx| + |dI/dy|)
        """
        B, C, H, W = img.shape
        dy = torch.abs(img[:, :, 1:, :] - img[:, :, :-1, :]).mean()
        dx = torch.abs(img[:, :, :, 1:] - img[:, :, :, :-1]).mean()
        return dy + dx

    def save_checkpoint(self, path, epoch=None, stage=None, val_metrics=None):
        """
        ä¿å­˜æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            path: ä¿å­˜è·¯å¾„
            epoch: å½“å‰ epoch (å¯é€‰)
            stage: å½“å‰è®­ç»ƒé˜¶æ®µ (å¯é€‰)
            val_metrics: éªŒè¯æŒ‡æ ‡ (å¯é€‰)
        """
        checkpoint = {
            'restoration_net': self.restoration_net.state_dict(),
            'optimizer_W': self.optimizer_W.state_dict(),
            'best_metrics': self.best_metrics,
        }
        if self.aberration_net is not None:
            checkpoint['aberration_net'] = self.aberration_net.state_dict()
        if self.optimizer_Theta is not None:
            checkpoint['optimizer_Theta'] = self.optimizer_Theta.state_dict()
        
        if epoch is not None:
            checkpoint['epoch'] = epoch
        if stage is not None:
            checkpoint['stage'] = stage
        if val_metrics is not None:
            checkpoint['val_metrics'] = val_metrics
            
        torch.save(checkpoint, path)
    
    def load_checkpoint(self, path, load_optimizer=True):
        """
        åŠ è½½æ¨¡å‹æ£€æŸ¥ç‚¹
        
        Args:
            path: æ£€æŸ¥ç‚¹è·¯å¾„
            load_optimizer: æ˜¯å¦åŠ è½½ä¼˜åŒ–å™¨çŠ¶æ€
        
        Returns:
            dict: æ£€æŸ¥ç‚¹ä¸­çš„é¢å¤–ä¿¡æ¯ (epoch, stage, val_metrics ç­‰)
        """
        checkpoint = torch.load(path, map_location=self.device)
        
        self.restoration_net.load_state_dict(checkpoint['restoration_net'])
        if self.aberration_net is not None and 'aberration_net' in checkpoint:
            self.aberration_net.load_state_dict(checkpoint['aberration_net'])
        
        if load_optimizer:
            if 'optimizer_W' in checkpoint:
                self.optimizer_W.load_state_dict(checkpoint['optimizer_W'])
            if self.optimizer_Theta is not None and 'optimizer_Theta' in checkpoint:
                self.optimizer_Theta.load_state_dict(checkpoint['optimizer_Theta'])
        
        if 'best_metrics' in checkpoint:
            self.best_metrics = checkpoint['best_metrics']
        
        return {
            'epoch': checkpoint.get('epoch'),
            'stage': checkpoint.get('stage'),
            'val_metrics': checkpoint.get('val_metrics')
        }
    
    def get_current_lr(self) -> Dict[str, float]:
        """è·å–å½“å‰å­¦ä¹ ç‡"""
        lr_optics = float('nan')
        if self.optimizer_Theta is not None:
            lr_optics = self.optimizer_Theta.param_groups[0]['lr']
        return {
            'lr_restoration': self.optimizer_W.param_groups[0]['lr'],
            'lr_optics': lr_optics
        }
